{"nbformat": 4, "nbformat_minor": 0, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}}, "cells": [{"cell_type": "markdown", "metadata": {"id": "view-in-github", "colab_type": "text"}, "source": ["<a href=\"https://colab.research.google.com/github/yahia-kplr/Google_AI/blob/main/Sentiment_analysis_twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]}, {"cell_type": "markdown", "source": ["## 0.Depencencies"], "metadata": {"id": "LN5U2aaAdxma"}}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "VTIFQAXdwrL4", "outputId": "1d704b79-c050-4eb0-c137-68db4b02adc5"}, "source": ["pip install nltk==3.3"], "execution_count": 1, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n", "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.7/dist-packages (3.3)\n", "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3) (1.15.0)\n"]}]}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "99SWc0q2w3w0", "outputId": "470535f9-0290-4918-d7d3-b80392165a00"}, "source": ["import nltk\n", "nltk.download('twitter_samples') # Le corpus Twitter de NLTK contient actuellement un \u00e9chantillon de 20k Tweets (nomm\u00e9s ' twitter_samples ') r\u00e9cup\u00e9r\u00e9s \u00e0 partir de l'API Twitter Streaming.\n", "nltk.download('punkt') # Punkt Sentence Tokenizer. Ce tokenizer divise un texte en une liste de phrases en utilisant un algorithme non supervis\u00e9 pour construire un mod\u00e8le pour les mots d'abr\u00e9viation, les collocations et les mots qui commencent les phrases. Il doit \u00eatre entra\u00een\u00e9 sur une grande collection de textes en clair dans la langue cible avant de pouvoir \u00eatre utilis\u00e9.\n", "nltk.download('wordnet') # WordNet est une base de donn\u00e9es lexicale pour la langue anglaise, qui a \u00e9t\u00e9 cr\u00e9\u00e9e par Princeton, et fait partie du corpus NLTK. Vous pouvez utiliser WordNet avec le module NLTK pour trouver le sens des mots, les synonymes, les antonymes, et plus encore. \n", "nltk.download('averaged_perceptron_tagger') #The averaged_perceptron_tagger.zip contains the pre-trained English https://en.wikipedia.org/wiki/Part_of_speech\n", "nltk.download('stopwords') #mots vides"], "execution_count": 2, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n", "[nltk_data]   Package twitter_samples is already up-to-date!\n", "[nltk_data] Downloading package punkt to /root/nltk_data...\n", "[nltk_data]   Package punkt is already up-to-date!\n", "[nltk_data] Downloading package wordnet to /root/nltk_data...\n", "[nltk_data]   Package wordnet is already up-to-date!\n", "[nltk_data] Downloading package averaged_perceptron_tagger to\n", "[nltk_data]     /root/nltk_data...\n", "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n", "[nltk_data]       date!\n", "[nltk_data] Downloading package stopwords to /root/nltk_data...\n", "[nltk_data]   Package stopwords is already up-to-date!\n"]}, {"output_type": "execute_result", "data": {"text/plain": ["True"]}, "metadata": {}, "execution_count": 2}]}, {"cell_type": "code", "source": ["ls /root/nltk_data/corpora"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "cQW7kAibe2SQ", "outputId": "6ee3d0c6-f6f4-4226-9281-3b284993b53a"}, "execution_count": 3, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["\u001b[0m\u001b[01;34mstopwords\u001b[0m/     \u001b[01;34mtwitter_samples\u001b[0m/     \u001b[01;34mwordnet\u001b[0m/\n", "stopwords.zip  twitter_samples.zip  wordnet.zip\n"]}]}, {"cell_type": "markdown", "source": ["\"wordnet\" doit \u00eatre d\u00e9compress\u00e9 manuellement"], "metadata": {"id": "YsCM9x5Zd5aF"}}, {"cell_type": "code", "source": ["from zipfile import ZipFile\n", "file_loc = '/root/nltk_data/corpora/wordnet.zip'\n", "with ZipFile(file_loc, 'r') as z:\n", "  z.extractall('/root/nltk_data/corpora/')"], "metadata": {"id": "mrd5-4hQeo7H"}, "execution_count": 4, "outputs": []}, {"cell_type": "code", "source": ["from nltk.stem.wordnet import WordNetLemmatizer\n", "from nltk.corpus import twitter_samples, stopwords\n", "from nltk.tag import pos_tag\n", "from nltk.tokenize import word_tokenize\n", "from nltk import FreqDist, classify, NaiveBayesClassifier\n", "\n", "import re, string, random"], "metadata": {"id": "PHMg0CKEZSlG"}, "execution_count": 5, "outputs": []}, {"cell_type": "markdown", "source": ["## 1.D\u00e9finition des fonctions de traitement de texte"], "metadata": {"id": "uUgiVHtxfJQi"}}, {"cell_type": "code", "source": ["def remove_noise(tweet_tokens, stop_words = ()):\n", "\n", "    cleaned_tokens = []\n", "\n", "    for token, tag in pos_tag(tweet_tokens):\n", "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n", "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n", "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n", "\n", "        if tag.startswith(\"NN\"):\n", "            pos = 'n'\n", "        elif tag.startswith('VB'):\n", "            pos = 'v'\n", "        else:\n", "            pos = 'a'\n", "\n", "        lemmatizer = WordNetLemmatizer()\n", "        token = lemmatizer.lemmatize(token, pos)\n", "\n", "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n", "            cleaned_tokens.append(token.lower())\n", "    return cleaned_tokens"], "metadata": {"id": "3cyrvJ9IZW-l"}, "execution_count": 6, "outputs": []}, {"cell_type": "code", "source": ["def get_all_words(cleaned_tokens_list):\n", "    for tokens in cleaned_tokens_list:\n", "        for token in tokens:\n", "            yield token"], "metadata": {"id": "o-WvmF1IZbL_"}, "execution_count": 7, "outputs": []}, {"cell_type": "code", "source": ["def get_tweets_for_model(cleaned_tokens_list):\n", "    for tweet_tokens in cleaned_tokens_list:\n", "        yield dict([token, True] for token in tweet_tokens)"], "metadata": {"id": "W0nLNnmAZdy-"}, "execution_count": 8, "outputs": []}, {"cell_type": "markdown", "source": ["## 2.Construire le dataset \u00e0 partir du corpus propre"], "metadata": {"id": "yC2jOHjyfcXg"}}, {"cell_type": "code", "source": ["positive_tweets = twitter_samples.strings('positive_tweets.json')\n", "negative_tweets = twitter_samples.strings('negative_tweets.json')\n", "\n", "text = twitter_samples.strings('tweets.20150430-223406.json')\n", "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]"], "metadata": {"id": "aMbR7ugybOY7"}, "execution_count": 9, "outputs": []}, {"cell_type": "code", "source": ["stop_words = stopwords.words('english')"], "metadata": {"id": "6D3khgpnbSrh"}, "execution_count": 10, "outputs": []}, {"cell_type": "code", "source": ["positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n", "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')"], "metadata": {"id": "HiT1K4QTbXwG"}, "execution_count": 11, "outputs": []}, {"cell_type": "code", "source": ["positive_cleaned_tokens_list = []\n", "negative_cleaned_tokens_list = []\n", "\n", "for tokens in positive_tweet_tokens:\n", "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n", "\n", "for tokens in negative_tweet_tokens:\n", "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"], "metadata": {"id": "rXZgw1pXbeLV"}, "execution_count": 12, "outputs": []}, {"cell_type": "code", "source": ["all_pos_words = get_all_words(positive_cleaned_tokens_list)\n", "\n", "freq_dist_pos = FreqDist(all_pos_words)\n", "print(freq_dist_pos.most_common(10))"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "zHMzPRpBbnBc", "outputId": "b254b5ae-79c3-4ab2-ab2e-d9595d1f5276"}, "execution_count": 13, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"]}]}, {"cell_type": "code", "source": ["positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n", "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n", "\n", "positive_dataset = [(tweet_dict, \"Positive\")\n", "                        for tweet_dict in positive_tokens_for_model]\n", "\n", "negative_dataset = [(tweet_dict, \"Negative\")\n", "                        for tweet_dict in negative_tokens_for_model]\n", "\n", "dataset = positive_dataset + negative_dataset\n", "\n", "random.shuffle(dataset) #m\u00e9langer arbitrairement\n", "\n"], "metadata": {"id": "hmO6lfBwbwfX"}, "execution_count": 14, "outputs": []}, {"cell_type": "markdown", "source": ["## 3.Le mod\u00e8le Naive Bayes classifier"], "metadata": {"id": "7ceirnNefzwU"}}, {"cell_type": "code", "source": ["train_data = dataset[:7000]\n", "test_data = dataset[7000:]"], "metadata": {"id": "YLms-IJEgFL5"}, "execution_count": 15, "outputs": []}, {"cell_type": "code", "source": ["classifier = NaiveBayesClassifier.train(train_data)\n", "\n", "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n", "print(classifier.show_most_informative_features(20))"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "-Fy9_Mf_b0hf", "outputId": "b676f5f2-cb77-434b-fd80-a7c1a11a75bd"}, "execution_count": 16, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Accuracy is: 0.9936666666666667\n", "Most Informative Features\n", "                follower = True           Positi : Negati =     33.1 : 1.0\n", "                     sad = True           Negati : Positi =     25.0 : 1.0\n", "                     x15 = True           Negati : Positi =     19.0 : 1.0\n", "                    damn = True           Negati : Positi =     15.0 : 1.0\n", "                      aw = True           Negati : Positi =     14.3 : 1.0\n", "                   cream = True           Negati : Positi =     13.6 : 1.0\n", "                     ice = True           Negati : Positi =     13.0 : 1.0\n", "                 welcome = True           Positi : Negati =     11.9 : 1.0\n", "                  arrive = True           Positi : Negati =     11.9 : 1.0\n", "                   sorry = True           Negati : Positi =     11.8 : 1.0\n", "                 awesome = True           Positi : Negati =     11.8 : 1.0\n", "                     ugh = True           Negati : Positi =     11.6 : 1.0\n", "                   didnt = True           Negati : Positi =     11.0 : 1.0\n", "                    miss = True           Negati : Positi =     10.7 : 1.0\n", "                     bro = True           Positi : Negati =     10.4 : 1.0\n", "                   shame = True           Negati : Positi =     10.3 : 1.0\n", "                followed = True           Negati : Positi =      9.8 : 1.0\n", "                 perfect = True           Positi : Negati =      9.7 : 1.0\n", "                       \ud83d\ude2d = True           Negati : Positi =      9.7 : 1.0\n", "                    kill = True           Negati : Positi =      9.7 : 1.0\n", "None\n"]}]}, {"cell_type": "markdown", "source": ["## 4.Utiliser le mod\u00e8le pour classer son tweet"], "metadata": {"id": "Ktr0OKkBgOn5"}}, {"cell_type": "code", "source": ["custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n", "\n", "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n", "\n", "print(custom_tweet, classifier.classify(dict([token, True] for token in custom_tokens)))"], "metadata": {"id": "jwzF9tiALWVX", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "4cef7d7c-80ee-49b6-953f-7c8c7a6886c8"}, "execution_count": 17, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["I ordered just once from TerribleCo, they screwed up, never used the app again. Negative\n"]}]}, {"cell_type": "markdown", "source": ["C'est \u00e0 vous maintenant de tester et challenger l'algorithme!"], "metadata": {"id": "o7cjV_jmh256"}}]}
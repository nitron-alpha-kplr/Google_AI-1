{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_analysis_twitter.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doudi0101/Google_AI/blob/main/Sentiment_analysis_twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.Depencencies"
      ],
      "metadata": {
        "id": "LN5U2aaAdxma"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTIFQAXdwrL4",
        "outputId": "1d704b79-c050-4eb0-c137-68db4b02adc5"
      },
      "source": [
        "pip install nltk==3.3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.7/dist-packages (3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99SWc0q2w3w0",
        "outputId": "470535f9-0290-4918-d7d3-b80392165a00"
      },
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples') # Le corpus Twitter de NLTK contient actuellement un √©chantillon de 20k Tweets (nomm√©s ' twitter_samples ') r√©cup√©r√©s √† partir de l'API Twitter Streaming.\n",
        "nltk.download('punkt') # Punkt Sentence Tokenizer. Ce tokenizer divise un texte en une liste de phrases en utilisant un algorithme non supervis√© pour construire un mod√®le pour les mots d'abr√©viation, les collocations et les mots qui commencent les phrases. Il doit √™tre entra√Æn√© sur une grande collection de textes en clair dans la langue cible avant de pouvoir √™tre utilis√©.\n",
        "nltk.download('wordnet') # WordNet est une base de donn√©es lexicale pour la langue anglaise, qui a √©t√© cr√©√©e par Princeton, et fait partie du corpus NLTK. Vous pouvez utiliser WordNet avec le module NLTK pour trouver le sens des mots, les synonymes, les antonymes, et plus encore. \n",
        "nltk.download('averaged_perceptron_tagger') #The averaged_perceptron_tagger.zip contains the pre-trained English https://en.wikipedia.org/wiki/Part_of_speech\n",
        "nltk.download('stopwords') #mots vides"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls /root/nltk_data/corpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQW7kAibe2SQ",
        "outputId": "6ee3d0c6-f6f4-4226-9281-3b284993b53a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mstopwords\u001b[0m/     \u001b[01;34mtwitter_samples\u001b[0m/     \u001b[01;34mwordnet\u001b[0m/\n",
            "stopwords.zip  twitter_samples.zip  wordnet.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"wordnet\" doit √™tre d√©compress√© manuellement"
      ],
      "metadata": {
        "id": "YsCM9x5Zd5aF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "file_loc = '/root/nltk_data/corpora/wordnet.zip'\n",
        "with ZipFile(file_loc, 'r') as z:\n",
        "  z.extractall('/root/nltk_data/corpora/')"
      ],
      "metadata": {
        "id": "mrd5-4hQeo7H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import twitter_samples, stopwords\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
        "\n",
        "import re, string, random"
      ],
      "metadata": {
        "id": "PHMg0CKEZSlG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.D√©finition des fonctions de traitement de texte"
      ],
      "metadata": {
        "id": "uUgiVHtxfJQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_noise(tweet_tokens, stop_words = ()):\n",
        "\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
        "\n",
        "        if tag.startswith(\"NN\"):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "\n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
        "            cleaned_tokens.append(token.lower())\n",
        "    return cleaned_tokens"
      ],
      "metadata": {
        "id": "3cyrvJ9IZW-l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_words(cleaned_tokens_list):\n",
        "    for tokens in cleaned_tokens_list:\n",
        "        for token in tokens:\n",
        "            yield token"
      ],
      "metadata": {
        "id": "o-WvmF1IZbL_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tweets_for_model(cleaned_tokens_list):\n",
        "    for tweet_tokens in cleaned_tokens_list:\n",
        "        yield dict([token, True] for token in tweet_tokens)"
      ],
      "metadata": {
        "id": "W0nLNnmAZdy-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Construire le dataset √† partir du corpus propre"
      ],
      "metadata": {
        "id": "yC2jOHjyfcXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "text = twitter_samples.strings('tweets.20150430-223406.json')\n",
        "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]"
      ],
      "metadata": {
        "id": "aMbR7ugybOY7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "6D3khgpnbSrh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')"
      ],
      "metadata": {
        "id": "HiT1K4QTbXwG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_cleaned_tokens_list = []\n",
        "negative_cleaned_tokens_list = []\n",
        "\n",
        "for tokens in positive_tweet_tokens:\n",
        "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
        "\n",
        "for tokens in negative_tweet_tokens:\n",
        "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
      ],
      "metadata": {
        "id": "rXZgw1pXbeLV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
        "\n",
        "freq_dist_pos = FreqDist(all_pos_words)\n",
        "print(freq_dist_pos.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHMzPRpBbnBc",
        "outputId": "b254b5ae-79c3-4ab2-ab2e-d9595d1f5276"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
        "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
        "\n",
        "positive_dataset = [(tweet_dict, \"Positive\")\n",
        "                        for tweet_dict in positive_tokens_for_model]\n",
        "\n",
        "negative_dataset = [(tweet_dict, \"Negative\")\n",
        "                        for tweet_dict in negative_tokens_for_model]\n",
        "\n",
        "dataset = positive_dataset + negative_dataset\n",
        "\n",
        "random.shuffle(dataset) #m√©langer arbitrairement\n",
        "\n"
      ],
      "metadata": {
        "id": "hmO6lfBwbwfX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Le mod√®le Naive Bayes classifier"
      ],
      "metadata": {
        "id": "7ceirnNefzwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = dataset[:7000]\n",
        "test_data = dataset[7000:]"
      ],
      "metadata": {
        "id": "YLms-IJEgFL5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = NaiveBayesClassifier.train(train_data)\n",
        "\n",
        "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
        "print(classifier.show_most_informative_features(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Fy9_Mf_b0hf",
        "outputId": "b676f5f2-cb77-434b-fd80-a7c1a11a75bd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is: 0.9936666666666667\n",
            "Most Informative Features\n",
            "                follower = True           Positi : Negati =     33.1 : 1.0\n",
            "                     sad = True           Negati : Positi =     25.0 : 1.0\n",
            "                     x15 = True           Negati : Positi =     19.0 : 1.0\n",
            "                    damn = True           Negati : Positi =     15.0 : 1.0\n",
            "                      aw = True           Negati : Positi =     14.3 : 1.0\n",
            "                   cream = True           Negati : Positi =     13.6 : 1.0\n",
            "                     ice = True           Negati : Positi =     13.0 : 1.0\n",
            "                 welcome = True           Positi : Negati =     11.9 : 1.0\n",
            "                  arrive = True           Positi : Negati =     11.9 : 1.0\n",
            "                   sorry = True           Negati : Positi =     11.8 : 1.0\n",
            "                 awesome = True           Positi : Negati =     11.8 : 1.0\n",
            "                     ugh = True           Negati : Positi =     11.6 : 1.0\n",
            "                   didnt = True           Negati : Positi =     11.0 : 1.0\n",
            "                    miss = True           Negati : Positi =     10.7 : 1.0\n",
            "                     bro = True           Positi : Negati =     10.4 : 1.0\n",
            "                   shame = True           Negati : Positi =     10.3 : 1.0\n",
            "                followed = True           Negati : Positi =      9.8 : 1.0\n",
            "                 perfect = True           Positi : Negati =      9.7 : 1.0\n",
            "                       üò≠ = True           Negati : Positi =      9.7 : 1.0\n",
            "                    kill = True           Negati : Positi =      9.7 : 1.0\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Utiliser le mod√®le pour classer son tweet"
      ],
      "metadata": {
        "id": "Ktr0OKkBgOn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
        "\n",
        "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
        "\n",
        "print(custom_tweet, classifier.classify(dict([token, True] for token in custom_tokens)))"
      ],
      "metadata": {
        "id": "jwzF9tiALWVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cef7d7c-80ee-49b6-953f-7c8c7a6886c8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I ordered just once from TerribleCo, they screwed up, never used the app again. Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "C'est √† vous maintenant de tester et challenger l'algorithme!"
      ],
      "metadata": {
        "id": "o7cjV_jmh256"
      }
    }
  ]
}